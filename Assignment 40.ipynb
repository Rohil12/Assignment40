{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb03679c-6502-4080-a713-e6f434f4a003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Filter Method in Feature Selection\\nDefinition: The filter method is a technique used in feature selection to identify and select features based on their statistical significance or intrinsic properties, independent of any specific machine learning algorithm. It helps to reduce the dimensionality of the data by removing irrelevant or redundant features before training a model.\\n\\nHow It Works:\\nScoring Criteria: Features are scored based on various statistical measures, such as correlation, mutual information, Chi-square test, ANOVA F-test, etc.\\n\\nRanking Features: Once scores are computed, features are ranked according to their scores.\\n\\nSelection Threshold: A threshold or top-k features are selected based on their ranks.\\n\\nCommon Scoring Methods:\\nCorrelation Coefficient: Measures the linear relationship between a feature and the target variable. Features with high correlation to the target are selected.\\n\\nExample: Pearson correlation.\\n\\nMutual Information: Measures the dependency between two variables. High mutual information indicates a strong dependency between the feature and the target.\\n\\nExample: Information gain.\\n\\nChi-Square Test: Used for categorical data to evaluate if distributions of categorical variables differ from expected distributions.\\n\\nExample: Chi-square statistic.\\n\\nANOVA F-test: Assesses whether the means of two or more groups are statistically different from each other.\\n\\nExample: F-statistic.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q.1\n",
    "\"\"\"Filter Method in Feature Selection\n",
    "Definition: The filter method is a technique used in feature selection to identify and select features based on their statistical significance or intrinsic properties, independent of any specific machine learning algorithm. It helps to reduce the dimensionality of the data by removing irrelevant or redundant features before training a model.\n",
    "\n",
    "How It Works:\n",
    "Scoring Criteria: Features are scored based on various statistical measures, such as correlation, mutual information, Chi-square test, ANOVA F-test, etc.\n",
    "\n",
    "Ranking Features: Once scores are computed, features are ranked according to their scores.\n",
    "\n",
    "Selection Threshold: A threshold or top-k features are selected based on their ranks.\n",
    "\n",
    "Common Scoring Methods:\n",
    "Correlation Coefficient: Measures the linear relationship between a feature and the target variable. Features with high correlation to the target are selected.\n",
    "\n",
    "Example: Pearson correlation.\n",
    "\n",
    "Mutual Information: Measures the dependency between two variables. High mutual information indicates a strong dependency between the feature and the target.\n",
    "\n",
    "Example: Information gain.\n",
    "\n",
    "Chi-Square Test: Used for categorical data to evaluate if distributions of categorical variables differ from expected distributions.\n",
    "\n",
    "Example: Chi-square statistic.\n",
    "\n",
    "ANOVA F-test: Assesses whether the means of two or more groups are statistically different from each other.\n",
    "\n",
    "Example: F-statistic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5ced13-65a8-424e-aaaa-9ab477cae286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Filter Method\\nDescription:\\n\\nThe Filter method selects features based on their intrinsic properties, using statistical measures without involving any machine learning algorithms.\\n\\nIt evaluates each feature individually against the target variable.\\n\\nHow it Works:\\n\\nScoring Criteria: Features are scored using statistical metrics like correlation, mutual information, Chi-square, etc.\\n\\nRanking: Features are ranked based on their scores.\\n\\nSelection: Top-ranked features are selected according to a threshold or number of features.\\n\\nPros:\\n\\nSimplicity: Easy to implement and understand.\\n\\nEfficiency: Fast, as it does not involve iterative model training.\\n\\nIndependence: Can be applied before model training.\\n\\nCons:\\n\\nIgnore Feature Interaction: Does not consider interactions between features.\\n\\nSuboptimal Performance: Might not yield the best set of features for a specific model.\\n\\nWrapper Method\\nDescription:\\n\\nThe Wrapper method selects features based on their performance with a specific machine learning algorithm.\\n\\nIt evaluates different subsets of features by training and testing the model iteratively.\\n\\nHow it Works:\\n\\nSearch Strategy: Uses techniques like forward selection, backward elimination, or recursive feature elimination.\\n\\nModel Training: Trains and evaluates the model with different feature subsets.\\n\\nSelection: Chooses the subset of features that yields the best model performance.\\n\\nPros:\\n\\nOptimal Feature Set: Can find the most relevant feature set for a specific model, considering feature interactions.\\n\\nModel-Specific: Tailors feature selection to the algorithm being used, often leading to better performance.\\n\\nCons:\\n\\nComputationally Intensive: Requires multiple rounds of model training, which can be time-consuming.\\n\\nOverfitting Risk: Higher risk of overfitting if the model is tuned too closely to the training data.\\n\\nExample:\\nFilter Method: You might use Pearson correlation to select features that have a high correlation with the target variable.\\n\\nWrapper Method: You might use recursive feature elimination with a decision tree classifier to iteratively remove the least important features and find the best subset.\\n\\nIn essence, while the Filter method is quick and works well for an initial feature reduction, the Wrapper method provides a more tailored feature set by iteratively testing the model's performance, albeit at a higher computational cost. It’s like the difference between a quick scan and a detailed\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.2\n",
    "\"\"\"Filter Method\n",
    "Description:\n",
    "\n",
    "The Filter method selects features based on their intrinsic properties, using statistical measures without involving any machine learning algorithms.\n",
    "\n",
    "It evaluates each feature individually against the target variable.\n",
    "\n",
    "How it Works:\n",
    "\n",
    "Scoring Criteria: Features are scored using statistical metrics like correlation, mutual information, Chi-square, etc.\n",
    "\n",
    "Ranking: Features are ranked based on their scores.\n",
    "\n",
    "Selection: Top-ranked features are selected according to a threshold or number of features.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Simplicity: Easy to implement and understand.\n",
    "\n",
    "Efficiency: Fast, as it does not involve iterative model training.\n",
    "\n",
    "Independence: Can be applied before model training.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Ignore Feature Interaction: Does not consider interactions between features.\n",
    "\n",
    "Suboptimal Performance: Might not yield the best set of features for a specific model.\n",
    "\n",
    "Wrapper Method\n",
    "Description:\n",
    "\n",
    "The Wrapper method selects features based on their performance with a specific machine learning algorithm.\n",
    "\n",
    "It evaluates different subsets of features by training and testing the model iteratively.\n",
    "\n",
    "How it Works:\n",
    "\n",
    "Search Strategy: Uses techniques like forward selection, backward elimination, or recursive feature elimination.\n",
    "\n",
    "Model Training: Trains and evaluates the model with different feature subsets.\n",
    "\n",
    "Selection: Chooses the subset of features that yields the best model performance.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Optimal Feature Set: Can find the most relevant feature set for a specific model, considering feature interactions.\n",
    "\n",
    "Model-Specific: Tailors feature selection to the algorithm being used, often leading to better performance.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Computationally Intensive: Requires multiple rounds of model training, which can be time-consuming.\n",
    "\n",
    "Overfitting Risk: Higher risk of overfitting if the model is tuned too closely to the training data.\n",
    "\n",
    "Example:\n",
    "Filter Method: You might use Pearson correlation to select features that have a high correlation with the target variable.\n",
    "\n",
    "Wrapper Method: You might use recursive feature elimination with a decision tree classifier to iteratively remove the least important features and find the best subset.\n",
    "\n",
    "In essence, while the Filter method is quick and works well for an initial feature reduction, the Wrapper method provides a more tailored feature set by iteratively testing the model's performance, albeit at a higher computational cost. It’s like the difference between a quick scan and a detailed\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbac63e-9097-4011-ad6e-de425a360fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Embedded Feature Selection Methods\\nEmbedded feature selection methods integrate the feature selection process into the model training. These methods take advantage of the learning algorithm itself to weigh and select features, often leading to more efficient and effective feature selection. Here are some common techniques:\\n\\nLasso Regularization (L1 Regularization):\\n\\nDescription: Lasso (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty to the loss function, which can shrink some feature coefficients to zero, effectively performing feature selection.\\n\\nExample: Lasso Regression\\n\\nRidge Regularization (L2 Regularization):\\n\\nDescription: Adds an L2 penalty to the loss function, which discourages large coefficients but does not shrink them to zero. It’s mainly used to prevent overfitting rather than feature selection.\\n\\nExample: Ridge Regression\\n\\nElastic Net Regularization:\\n\\nDescription: Combines L1 and L2 penalties to balance between feature selection and model complexity.\\n\\nExample: Elastic Net Regression\\n\\nDecision Trees and Random Forests:\\n\\nDescription: Tree-based algorithms naturally perform feature selection by selecting features that split the data to maximize information gain or Gini impurity. Features with the highest importance scores can be selected.\\n\\nExample: Decision Tree, Random Forest\\n\\nGradient Boosting Machines (GBM):\\n\\nDescription: Uses an ensemble of weak learners (typically decision trees) to improve prediction accuracy. Feature importance can be derived from the contribution of each feature to the model’s accuracy.\\n\\nExample: XGBoost, LightGBM, CatBoost\\n\\nRecursive Feature Elimination (RFE):\\n\\nDescription: Recursively removes the least important features and builds the model on the remaining features to identify the most significant ones.\\n\\nExample: RFE with Support Vector Machines (SVM), RFE with Logistic Regression'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q.3\n",
    "\"\"\"Embedded Feature Selection Methods\n",
    "Embedded feature selection methods integrate the feature selection process into the model training. These methods take advantage of the learning algorithm itself to weigh and select features, often leading to more efficient and effective feature selection. Here are some common techniques:\n",
    "\n",
    "Lasso Regularization (L1 Regularization):\n",
    "\n",
    "Description: Lasso (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty to the loss function, which can shrink some feature coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "Example: Lasso Regression\n",
    "\n",
    "Ridge Regularization (L2 Regularization):\n",
    "\n",
    "Description: Adds an L2 penalty to the loss function, which discourages large coefficients but does not shrink them to zero. It’s mainly used to prevent overfitting rather than feature selection.\n",
    "\n",
    "Example: Ridge Regression\n",
    "\n",
    "Elastic Net Regularization:\n",
    "\n",
    "Description: Combines L1 and L2 penalties to balance between feature selection and model complexity.\n",
    "\n",
    "Example: Elastic Net Regression\n",
    "\n",
    "Decision Trees and Random Forests:\n",
    "\n",
    "Description: Tree-based algorithms naturally perform feature selection by selecting features that split the data to maximize information gain or Gini impurity. Features with the highest importance scores can be selected.\n",
    "\n",
    "Example: Decision Tree, Random Forest\n",
    "\n",
    "Gradient Boosting Machines (GBM):\n",
    "\n",
    "Description: Uses an ensemble of weak learners (typically decision trees) to improve prediction accuracy. Feature importance can be derived from the contribution of each feature to the model’s accuracy.\n",
    "\n",
    "Example: XGBoost, LightGBM, CatBoost\n",
    "\n",
    "Recursive Feature Elimination (RFE):\n",
    "\n",
    "Description: Recursively removes the least important features and builds the model on the remaining features to identify the most significant ones.\n",
    "\n",
    "Example: RFE with Support Vector Machines (SVM), RFE with Logistic Regression\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390ba59a-beee-4930-a703-6e01c56644c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lasso Regularization: Adds L1 penalty, effectively performing feature selection by setting some coefficients to zero.\\n\\nFeature Importance: Selected features have non-zero coefficients, indicating their importance in the model.\\n\\nEmbedded methods are particularly powerful because they harness the model’s learning process to select the most relevant features, often resulting in better performance and efficiency. It's like getting a two-for-one deal—model training and feature selection in one go. Neat, right? \\n\\nLet me know if you’d like to dive deeper into any of these methods!\\n\\nWhat are some drawbacks of using the Filter method for feature selection?\\nThe Filter method, while popular and efficient for feature selection, does have its limitations. Here are some common drawbacks:\\n\\n1. Ignoring Feature Interaction:\\nIssue: Filter methods evaluate each feature independently, without considering how features interact with each other.\\n\\nImpact: Important combinations of features that only show their relevance together might be overlooked.\\n\\n2. Model-Agnostic Nature:\\nIssue: Since filter methods are not tied to a specific learning algorithm, they might not align perfectly with the model’s requirements.\\n\\nImpact: The selected features may not be the most optimal for the particular machine learning model being used, potentially leading to suboptimal performance.\\n\\n3. Static Criteria:\\nIssue: The statistical measures used (like correlation, mutual information, etc.) are static and do not adapt based on the model’s learning process.\\n\\nImpact: These measures might not fully capture the relevance of features in the context of the model’s performance.\\n\\n4. Risk of Oversimplification:\\nIssue: Filter methods might oversimplify the problem by relying solely on statistical properties.\\n\\nImpact: This can lead to the exclusion of features that, while not individually significant, could be important in conjunction with other features.\\n\\n5. Less Effective with Large Feature Sets:\\nIssue: With very high-dimensional data, the effectiveness of filter methods can diminish as they may not adequately handle the complexity of the feature interactions.\\n\\nImpact: This can result in suboptimal feature sets that do not improve model performance significantly.\\n\\nExample:\\nImagine a dataset where two features, when considered together, provide critical information about the target variable, but individually, they do not show significant correlation with the target. A filter method may disregard these features, missing out on their combined importance.\\n\\nIn summary, while filter methods are fast and easy to use, they might not always provide the best feature set for complex problems, especially where feature interactions play a crucial role. It’s like judging a book by its cover—sometimes, you need to delve deeper to understand its true value\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q.4\n",
    "\n",
    "\"\"\"Lasso Regularization: Adds L1 penalty, effectively performing feature selection by setting some coefficients to zero.\n",
    "\n",
    "Feature Importance: Selected features have non-zero coefficients, indicating their importance in the model.\n",
    "\n",
    "Embedded methods are particularly powerful because they harness the model’s learning process to select the most relevant features, often resulting in better performance and efficiency. It's like getting a two-for-one deal—model training and feature selection in one go. Neat, right? \n",
    "\n",
    "Let me know if you’d like to dive deeper into any of these methods!\n",
    "\n",
    "What are some drawbacks of using the Filter method for feature selection?\n",
    "The Filter method, while popular and efficient for feature selection, does have its limitations. Here are some common drawbacks:\n",
    "\n",
    "1. Ignoring Feature Interaction:\n",
    "Issue: Filter methods evaluate each feature independently, without considering how features interact with each other.\n",
    "\n",
    "Impact: Important combinations of features that only show their relevance together might be overlooked.\n",
    "\n",
    "2. Model-Agnostic Nature:\n",
    "Issue: Since filter methods are not tied to a specific learning algorithm, they might not align perfectly with the model’s requirements.\n",
    "\n",
    "Impact: The selected features may not be the most optimal for the particular machine learning model being used, potentially leading to suboptimal performance.\n",
    "\n",
    "3. Static Criteria:\n",
    "Issue: The statistical measures used (like correlation, mutual information, etc.) are static and do not adapt based on the model’s learning process.\n",
    "\n",
    "Impact: These measures might not fully capture the relevance of features in the context of the model’s performance.\n",
    "\n",
    "4. Risk of Oversimplification:\n",
    "Issue: Filter methods might oversimplify the problem by relying solely on statistical properties.\n",
    "\n",
    "Impact: This can lead to the exclusion of features that, while not individually significant, could be important in conjunction with other features.\n",
    "\n",
    "5. Less Effective with Large Feature Sets:\n",
    "Issue: With very high-dimensional data, the effectiveness of filter methods can diminish as they may not adequately handle the complexity of the feature interactions.\n",
    "\n",
    "Impact: This can result in suboptimal feature sets that do not improve model performance significantly.\n",
    "\n",
    "Example:\n",
    "Imagine a dataset where two features, when considered together, provide critical information about the target variable, but individually, they do not show significant correlation with the target. A filter method may disregard these features, missing out on their combined importance.\n",
    "\n",
    "In summary, while filter methods are fast and easy to use, they might not always provide the best feature set for complex problems, especially where feature interactions play a crucial role. It’s like judging a book by its cover—sometimes, you need to delve deeper to understand its true value\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaaea694-8ecd-4bd9-a43f-8b5e91f69950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      price  rating  delivery_time\n",
      "0  0.000000    0.25           0.50\n",
      "1  0.166667    0.50           0.25\n",
      "2  0.333333    0.75           0.00\n",
      "3  0.666667    1.00           1.00\n",
      "4  1.000000    0.00           0.75\n"
     ]
    }
   ],
   "source": [
    "# Q.5\n",
    "\"\"\"Min-Max Scaling:\n",
    "Definition: Min-Max scaling, also known as normalization, transforms features to a fixed range, usually [0, 1]. This technique ensures that no feature dominates due to its scale, facilitating better model performance.\n",
    "\n",
    "Steps to Apply Min-Max Scaling:\n",
    "Identify Features:\n",
    "\n",
    "Features: Price, Rating, Delivery Time.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {'price':[10,15,20,30,40],\n",
    "        'rating':[3.5,4.0,4.5,5.0,3.0],\n",
    "        'delivery_time':[30,25,20,40,35]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cdd043-4597-4bd7-b861-557689a101c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [1.00000000e+00 4.75826094e-33]\n",
      "Principal Components DataFrame:\n",
      "            PC1           PC2\n",
      "0 -2.828427e+00  3.244058e-17\n",
      "1 -1.414214e+00  8.422400e-17\n",
      "2  2.886580e-16 -1.657566e-17\n",
      "3  1.414214e+00 -2.267688e-16\n",
      "4  2.828427e+00  1.879370e-16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "Definition: Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by transforming the original features into a new set of uncorrelated features called principal components. These components are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "Why Use PCA in Stock Price Prediction?\n",
    "Reduce Complexity: Simplifies the model by reducing the number of features, making it easier to interpret.\n",
    "\n",
    "Improve Performance: Speeds up the training process and reduces the risk of overfitting.\n",
    "\n",
    "Capture Variability: Retains most of the original data's information in fewer dimensions.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "# Replace this with your actual dataset\n",
    "data = {\n",
    "    'financial_metric1': [1.2, 2.3, 3.4, 4.5, 5.6],\n",
    "    'financial_metric2': [2.2, 3.3, 4.4, 5.5, 6.6],\n",
    "    'market_trend1': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
    "    'market_trend2': [1.1, 2.1, 3.1, 4.1, 5.1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Choose the number of components or variance threshold\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Display the results\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Principal Components DataFrame:\")\n",
    "df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "print(df_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65787181-e542-4ce2-8b04-e3f2d4da8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Q.7\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the desired range\n",
    "a, b = -1, 1\n",
    "\n",
    "# Min-Max Scaling function\n",
    "def min_max_scaling(X, new_min, new_max):\n",
    "    X_min = np.min(X)\n",
    "    X_max = np.max(X)\n",
    "    X_scaled = (X - X_min) / (X_max - X_min) * (new_max - new_min) + new_min\n",
    "    return X_scaled\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaled_data = min_max_scaling(data, a, b)\n",
    "\n",
    "# Display the original and scaled data\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data:\", scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab59468-fbb7-4377-ae56-21e7815c3939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
